{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import grapl.algorithms as algs\n",
    "import grapl.dsl as dsl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse._data import _data_matrix\n",
    "\n",
    "causal_graph = '\"Alcohol Consumption Causal Graph\"; \\\n",
    "                D; F; S; A; P; M; N; G; H; \\\n",
    "                F -> D; \\\n",
    "                S -> D; \\\n",
    "                A -> D; \\\n",
    "                P -> D; \\\n",
    "                M -> D; \\\n",
    "                N -> D; \\\n",
    "                P -> F; \\\n",
    "                M -> F; \\\n",
    "                N -> F; \\\n",
    "                A -> H; \\\n",
    "                H -> D; \\\n",
    "                H -> G; \\\n",
    "                G -> D; '\n",
    "\n",
    "grapl_obj = dsl.GraplDSL()\n",
    "G = grapl_obj.readgrapl(causal_graph)\n",
    "# Dalc(D); Famrel(F); Sex(S); Age(A); Pstatus(P); Medu(M); Fedu(N); Mjob(J); Fjob(K); Goout(G); Health(H)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "id_str, id_eqn, isident = algs.idfixing(G, {'A'}, {'D'})"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e1615c44b1012a47",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      sex  age  Pstatus  Medu  Fedu  famrel  goout  health  Dalc  Walc\n0     0.0   18      0.0     4     4       4      4       3     1     1\n1     0.0   17      1.0     1     1       5      3       3     1     1\n2     0.0   15      1.0     1     1       4      2       3     2     3\n3     0.0   15      1.0     4     2       3      2       5     1     1\n4     0.0   16      1.0     3     3       4      2       5     1     2\n...   ...  ...      ...   ...   ...     ...    ...     ...   ...   ...\n1039  0.0   19      1.0     2     3       5      2       5     1     2\n1040  0.0   18      1.0     3     1       4      4       1     1     1\n1041  0.0   18      1.0     1     1       1      1       5     1     1\n1042  1.0   17      1.0     3     1       2      5       2     3     4\n1043  1.0   18      1.0     3     2       4      1       5     3     4\n\n[1044 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sex</th>\n      <th>age</th>\n      <th>Pstatus</th>\n      <th>Medu</th>\n      <th>Fedu</th>\n      <th>famrel</th>\n      <th>goout</th>\n      <th>health</th>\n      <th>Dalc</th>\n      <th>Walc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>18</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>17</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1039</th>\n      <td>0.0</td>\n      <td>19</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1040</th>\n      <td>0.0</td>\n      <td>18</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1041</th>\n      <td>0.0</td>\n      <td>18</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1042</th>\n      <td>1.0</td>\n      <td>17</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1043</th>\n      <td>1.0</td>\n      <td>18</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>1044 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "total_data = pd.read_csv(\"./student-encoded.csv\")\n",
    "# total_data.drop(labels = ['goout', 'health'], axis=1)\n",
    "# Specify the desired features and target column\n",
    "desired_features = ['sex', 'age', 'Pstatus','Medu','Fedu','famrel' , 'goout', 'health', 'Dalc','Walc']\n",
    "df = total_data[desired_features].copy()\n",
    "\n",
    "# df = pd.get_dummies(df, columns=['Mjob', 'Fjob'])\n",
    "\n",
    "# target_column = df['Walc']\n",
    "target_column = df['Dalc']\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:30:42.001013Z",
     "start_time": "2024-04-20T16:30:41.947171Z"
    }
   },
   "id": "1a0cc3b833c927c5",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: sex\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: age\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: Pstatus\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: Medu\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: Fedu\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: famrel\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: goout\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: health\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: Dalc\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n",
      "Feature: Walc\n",
      "Training set size: 835\n",
      "Testing set size: 209\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Create a file to save the split data\n",
    "output_dir = \"./split_data/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Iterate over each feature and split the dataset\n",
    "for column in df.columns:\n",
    "    # Get the data for the current feature\n",
    "    feature_data = df[[column]]\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_data, target_column, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Save the split training and testing sets as CSV files\n",
    "    train_filename = os.path.join(output_dir, f\"{column}_train.csv\")\n",
    "    test_filename = os.path.join(output_dir, f\"{column}_test.csv\")\n",
    "    X_train.to_csv(train_filename, index=False)\n",
    "    X_test.to_csv(test_filename, index=False)\n",
    "    \n",
    "    # Print the sizes of training and testing sets for each feature\n",
    "    print(f\"Feature: {column}\")\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Testing set size: {len(X_test)}\")\n",
    "    print(\"-----------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:30:49.700146Z",
     "start_time": "2024-04-20T16:30:49.616447Z"
    }
   },
   "id": "f2f339a23cef30e6",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "testdata_dir = \"./split_data/\"\n",
    "D_train = pd.read_csv(testdata_dir + \"Dalc_train.csv\")\n",
    "# D_train = pd.read_csv(testdata_dir + \"Walc_train.csv\")\n",
    "F_train = pd.read_csv(testdata_dir + \"famrel_train.csv\")\n",
    "S_train = pd.read_csv(testdata_dir + \"sex_train.csv\")\n",
    "A_train = pd.read_csv(testdata_dir + \"age_train.csv\")\n",
    "P_train = pd.read_csv(testdata_dir + \"Pstatus_train.csv\")\n",
    "M_train = pd.read_csv(testdata_dir + \"Medu_train.csv\")\n",
    "N_train = pd.read_csv(testdata_dir + \"Fedu_train.csv\")\n",
    "H_train = pd.read_csv(testdata_dir + \"health_train.csv\")\n",
    "G_train = pd.read_csv(testdata_dir + \"goout_train.csv\")\n",
    "\n",
    "# Jathome_train = pd.read_csv(testdata_dir + \"Mjob_at_home_train.csv\")\n",
    "# Jhealth_train = pd.read_csv(testdata_dir + \"Mjob_health_train.csv\")\n",
    "# Jother_train = pd.read_csv(testdata_dir + \"Mjob_other_train.csv\")\n",
    "# Jservices_train = pd.read_csv(testdata_dir + \"Mjob_services_train.csv\")\n",
    "# Jteacher_train = pd.read_csv(testdata_dir + \"Mjob_teacher_train.csv\")\n",
    "# Kathome_train = pd.read_csv(testdata_dir + \"Fjob_at_home_train.csv\")\n",
    "# Khealth_train = pd.read_csv(testdata_dir + \"Fjob_health_train.csv\")\n",
    "# Kother_train = pd.read_csv(testdata_dir + \"Fjob_other_train.csv\")\n",
    "# Kservices_train = pd.read_csv(testdata_dir + \"Fjob_services_train.csv\")\n",
    "# Kteacher_train = pd.read_csv(testdata_dir + \"Fjob_teacher_train.csv\")\n",
    "\n",
    "# J_train = pd.concat([Jathome_train, Jhealth_train, Jother_train, Jservices_train, Jteacher_train], axis=1)\n",
    "# K_train = pd.concat([Kathome_train, Khealth_train, Kother_train, Kservices_train, Kteacher_train], axis=1)\n",
    "\n",
    "# Reform the data to the acceptable format for the causalbootstrapping interfaces\n",
    "D_train = np.array(D_train)\n",
    "F_train = np.array(F_train)\n",
    "S_train = np.array(S_train)\n",
    "A_train = np.array(A_train)\n",
    "P_train = np.array(P_train)\n",
    "M_train = np.array(M_train)\n",
    "N_train = np.array(N_train)\n",
    "# J_train = np.array(J_train)\n",
    "# K_train = np.array(K_train)\n",
    "H_train = np.array(H_train)\n",
    "G_train = np.array(G_train)\n",
    "\n",
    "data = {\"D'\": D_train,\n",
    "        \"F\": F_train,\n",
    "        \"S\": S_train,\n",
    "        \"A\": A_train,\n",
    "        \"P\": P_train,\n",
    "        \"M\": M_train,\n",
    "        \"N\": N_train,\n",
    "        \"H\": H_train,\n",
    "        \"G\": G_train}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:31:08.119466Z",
     "start_time": "2024-04-20T16:31:08.087189Z"
    }
   },
   "id": "5e9700534a034513",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"p_{A}(D)=\\\\sum_{N,G,H,F,P',N',P,M,A',S}[p(G,H,A,D,S|N,P,M,F)p(G,A',H,S|P',N,M,F)p(G,S,H,A|F,N',P,M)p(G,H,A|N,P,M,F)p(N,G,P,M,S|H,A)p(N,F,P,A,S|M)p(G,N,P,M|H,A)p(N,P,M|H,A)p(N,M|P)p(P',N|M)p(N,M|P)p(N',M|P)p(H)p(H)p(A')p(H)p(H)p(H)/p(G,H)p(G,H)p(A',H)p(G,H)p(G,H)p(H,A)p(H,A)p(G,H)p(A)p(S)p(N)p(M)p(N)p(P)p(N)p(A)p(M)p(S)p(M)p(N)p(N)p(S)p(P)p(S)p(M)p(N)p(P)p(M)]\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:31:08.788446Z",
     "start_time": "2024-04-20T16:31:08.779562Z"
    }
   },
   "id": "4f023054efb34952",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['(G,H,A|N,P,M,F)',\n '(G,N,P,M|H,A)',\n '(H)',\n \"(P',N|M)\",\n \"(G,S,H,A|F,N',P,M)\",\n '(N,P,M|H,A)',\n \"(A')\",\n '(N,M|P)',\n '(N)',\n '(N,F,P,A,S|M)',\n '(N,G,P,M,S|H,A)',\n \"(N',M|P)\",\n '(H)/',\n '(A)',\n '(G,H)',\n '(P)',\n '(S)',\n \"(G,A',H,S|P',N,M,F)\",\n '(G,H,A,D,S|N,P,M,F)',\n \"(A',H)\",\n '(M)',\n '(H,A)']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = list(set(id_str.split('}')[2][1:-1].split('p')[1:]))\n",
    "prob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:31:09.586609Z",
     "start_time": "2024-04-20T16:31:09.562182Z"
    }
   },
   "id": "1efb3cd2a61bec59",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "22 22\n",
      "['(G,H,A|N,P,M,F)', '(G,N,P,M|H,A)', '(H)', '(P,N|M)', '(G,S,H,A|F,N,P,M)', '(N,P,M|H,A)', '(A)', '(N,M|P)', '(N)', '(N,F,P,A,S|M)', '(N,G,P,M,S|H,A)', '(N,M|P)', '(H)/', '(A)', '(G,H)', '(P)', '(S)', '(G,A,H,S|P,N,M,F)', '(G,H,A,D,S|N,P,M,F)', '(A,H)', '(M)', '(H,A)']\n",
      "['(G,H,A|N,P,M,F)', '(G,N,P,M|H,A)', '(H)', \"(P',N|M)\", \"(G,S,H,A|F,N',P,M)\", '(N,P,M|H,A)', \"(A')\", '(N,M|P)', '(N)', '(N,F,P,A,S|M)', '(N,G,P,M,S|H,A)', \"(N',M|P)\", '(H)/', '(A)', '(G,H)', '(P)', '(S)', \"(G,A',H,S|P',N,M,F)\", '(G,H,A,D,S|N,P,M,F)', \"(A',H)\", '(M)', '(H,A)']\n"
     ]
    }
   ],
   "source": [
    "prob_mod = prob.copy()\n",
    "print(len(prob_mod), len(prob))\n",
    "for i in range(len(prob)):\n",
    "    if \"'\" in prob_mod[i]:\n",
    "        prob_mod[i] = prob_mod[i].replace(\"'\", '')\n",
    "print(len(prob_mod), len(prob))\n",
    "print(prob_mod)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:31:12.932041Z",
     "start_time": "2024-04-20T16:31:12.905494Z"
    }
   },
   "id": "dc52a1811236185c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ['(G,H,A|N,P,M,F)', '(G,N,P,M|H,A)', '(H)', '(P,N|M)', '(G,S,H,A|F,N,P,M)', '(N,P,M|H,A)', '(A)', '(N,M|P)', '(N)', '(N,F,P,A,S|M)', '(N,G,P,M,S|H,A)', '(N,M|P)', '(H)/', '(A)', '(G,H)', '(P)', '(S)', '(G,A,H,S|P,N,M,F)', '(G,H,A,D,S|N,P,M,F)', '(A,H)', '(M)', '(H,A)']\n",
      "33 ['(G,H,A,N,P,M,F)', '(G,N,P,M,H,A)', '(H)', '(P,N,M)', '(G,S,H,A,F,N,P,M)', '(N,P,M,H,A)', '(A)', '(N,M,P)', '(N)', '(N,F,P,A,S,M)', '(N,G,P,M,S,H,A)', '(N,M,P)', '(H)/', '(A)', '(G,H)', '(P)', '(S)', '(G,A,H,S,P,N,M,F)', '(G,H,A,D,S,N,P,M,F)', '(A,H)', '(M)', '(H,A)', '(N,P,M,F)', '(H,A)', '(M)', '(F,N,P,M)', '(H,A)', '(P)', '(M)', '(H,A)', '(P)', '(P,N,M,F)', '(N,P,M,F)']\n"
     ]
    }
   ],
   "source": [
    "print(len(prob_mod), prob_mod)\n",
    "for i in range(len(prob_mod)):\n",
    "    if '|' in prob_mod[i]:\n",
    "        aux = prob_mod[i]\n",
    "        prob_mod[i] = aux.split('|')[0] + ',' + aux.split('|')[1]\n",
    "        denom = '(' + aux.split('|')[1]\n",
    "        # elem_ind = prob_mod.index(prob_mod[i])\n",
    "        # prob_mod.append(num)\n",
    "        prob_mod.append(denom)\n",
    "        # prob_mod.pop(elem_ind)\n",
    "print(len(prob_mod), prob_mod)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:31:15.704625Z",
     "start_time": "2024-04-20T16:31:15.658452Z"
    }
   },
   "id": "7f908b5ef7d9a1be",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['(N,F,P,A,S,M)',\n '(G,H,A,N,P,M,F)',\n '(H)',\n '(N,M,P)',\n '(A,H)',\n '(G,N,P,M,H,A)',\n '(N)',\n '(G,A,H,S,P,N,M,F)',\n '(P,N,M,F)',\n '(F,N,P,M)',\n '(N,P,M,H,A)',\n '(G,S,H,A,F,N,P,M)',\n '(H)/',\n '(A)',\n '(G,H)',\n '(P)',\n '(S)',\n '(P,N,M)',\n '(M)',\n '(N,G,P,M,S,H,A)',\n '(H,A)',\n '(G,H,A,D,S,N,P,M,F)',\n '(N,P,M,F)']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_mod = list(set(prob_mod))\n",
    "prob_mod"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:31:16.592705Z",
     "start_time": "2024-04-20T16:31:16.561578Z"
    }
   },
   "id": "235c49e4728cf0f2",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(N,F,P,A,S,M)\n",
      "(G,H,A,N,P,M,F)\n",
      "(H)\n",
      "(N,M,P)\n",
      "(A,H)\n",
      "(G,N,P,M,H,A)\n",
      "(N)\n",
      "(G,A,H,S,P,N,M,F)\n",
      "(P,N,M,F)\n",
      "(F,N,P,M)\n",
      "(N,P,M,H,A)\n",
      "(G,S,H,A,F,N,P,M)\n",
      "(H)/\n",
      "(A)\n",
      "(G,H)\n",
      "(P)\n",
      "(S)\n",
      "(P,N,M)\n",
      "(M)\n",
      "(N,G,P,M,S,H,A)\n",
      "(H,A)\n",
      "(G,H,A,D,S,N,P,M,F)\n",
      "(N,P,M,F)\n"
     ]
    }
   ],
   "source": [
    "import causalBootstrapping as cb\n",
    "from distfit import distfit\n",
    "from distEst_lib import MultivarContiDistributionEstimator\n",
    "dict_prob = {}\n",
    "for i in range(len(prob_mod)):\n",
    "    print(prob_mod[i])\n",
    "    n_bins = []\n",
    "    vars = ''\n",
    "    trains = ''\n",
    "    variables = prob_mod[i].split('(')[1].split(')')[0].split(',')\n",
    "    # print(len(variables))\n",
    "    for i in range(len(variables)):\n",
    "        if variables[i] in ['J', 'K']:\n",
    "            n_bins.extend([0,0,0,0,0])\n",
    "        elif variables[i] in ['A']:\n",
    "            n_bins.append(3)\n",
    "        else:\n",
    "            n_bins.append(0)\n",
    "            \n",
    "        if i+1 == len(variables):\n",
    "            trains += variables[i] + '_train' \n",
    "        else:\n",
    "            trains += variables[i] + '_train,' \n",
    "        vars += variables[i]\n",
    "    \n",
    "    if len(variables) > 1:\n",
    "        data = np.concatenate(((eval(trains))), axis=1)\n",
    "    else:\n",
    "        data = eval(trains)\n",
    "    # print(data)\n",
    "    # print(n_bins)\n",
    "    # dist = distfit()\n",
    "    # dist.fit_transform(data)\n",
    "    # print(dist.summary)\n",
    "    dist = MultivarContiDistributionEstimator(data_fit=data, n_bins = n_bins)\n",
    "    exec(\"pdf_\" + vars + \", p\" + vars + \"= dist.fit_histogram()\")\n",
    "    dict_prob[vars] = dist.fit_histogram()\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:39:03.664245Z",
     "start_time": "2024-04-20T16:39:02.957863Z"
    }
   },
   "id": "8a5dfae3201be811",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"p_{A}(D)=\\\\sum_{N,G,H,F,P',N',P,M,A',S}[p(G,H,A,D,S|N,P,M,F)p(G,A',H,S|P',N,M,F)p(G,S,H,A|F,N',P,M)p(G,H,A|N,P,M,F)p(N,G,P,M,S|H,A)p(N,F,P,A,S|M)p(G,N,P,M|H,A)p(N,P,M|H,A)p(N,M|P)p(P',N|M)p(N,M|P)p(N',M|P)p(H)p(H)p(A')p(H)p(H)p(H)/p(G,H)p(G,H)p(A',H)p(G,H)p(G,H)p(H,A)p(H,A)p(G,H)p(A)p(S)p(N)p(M)p(N)p(P)p(N)p(A)p(M)p(S)p(M)p(N)p(N)p(S)p(P)p(S)p(M)p(N)p(P)p(M)]\""
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:32:06.132438Z",
     "start_time": "2024-04-20T16:32:06.091939Z"
    }
   },
   "id": "d70e2612c701af15",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1131dc750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/monica/Desktop/DSGP8/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1131dc750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/monica/Desktop/DSGP8/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "np.dot(pGHADSNPMF,pGAHSPNMF)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-20T16:41:17.848529Z"
    }
   },
   "id": "3c9287c3fd3f84df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['NFPASM', 'GHANPMF', 'H', 'NMP', 'AH', 'GNPMHA', 'N', 'GAHSPNMF', 'PNMF', 'FNPM', 'NPMHA', 'GSHAFNPM', 'A', 'GH', 'P', 'S', 'PNM', 'M', 'NGPMSHA', 'HA', 'GHADSNPMF', 'NPMF'])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_prob.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:32:36.777724Z",
     "start_time": "2024-04-20T16:32:36.697015Z"
    }
   },
   "id": "af2f109737da3238",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['(N,F,P,A,S,M)',\n '(G,H,A,N,P,M,F)',\n '(H)',\n '(N,M,P)',\n '(A,H)',\n '(G,N,P,M,H,A)',\n '(N)',\n '(G,A,H,S,P,N,M,F)',\n '(P,N,M,F)',\n '(F,N,P,M)',\n '(N,P,M,H,A)',\n '(G,S,H,A,F,N,P,M)',\n '(H)/',\n '(A)',\n '(G,H)',\n '(P)',\n '(S)',\n '(P,N,M)',\n '(M)',\n '(N,G,P,M,S,H,A)',\n '(H,A)',\n '(G,H,A,D,S,N,P,M,F)',\n '(N,P,M,F)']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_mod"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T16:36:49.641693Z",
     "start_time": "2024-04-20T16:36:49.615352Z"
    }
   },
   "id": "e0dc18ab8ed3a4ff",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# dist = MultivarContiDistributionEstimator(data_fit=data, n_bins = n_bins)\n",
    "    # var_name = \"data_\" + vars\n",
    "    # dist_name = \"dist_estimator_\" + vars\n",
    "    # print(trains)\n",
    "    # if len(variables) == 1:\n",
    "    #     print(variables)\n",
    "    #     code = \"data_\" + vars + \" = \" + trains + '\\n'\n",
    "    # else:\n",
    "    #     code = \"data_\" + vars + \" = np.concatenate((\" + trains + '), axis = 1) \\n'\n",
    "    # \n",
    "    # code += \"dist_estimator_\" + vars + \" = MultivarContiDistributionEstimator(data_fit=\" + var_name + \",n_bins = \" + str(n_bins) + ') \\n'\n",
    "    # \n",
    "    # code += \"pdf_\" + vars + \", p\" + vars + \"= \" + dist_name + \".fit_histogram()\"\n",
    "    # exec(code)\n",
    "    # print(code)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7078e0aece567c5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
